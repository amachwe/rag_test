{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain, HuggingFaceHub\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from sentence_transformers.util import dot_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using ORCA2-13b and FALCON-7b\n",
    "ORCA_MODEL_PATH=\"C:\\\\Users\\\\azaha\\\\AppData\\\\Local\\\\nomic.ai\\\\GPT4All\\\\orca-2-13b.Q4_0.gguf\"\n",
    "FALCON_MODEL_PATH=\"C:\\\\Users\\\\azaha\\\\AppData\\\\Local\\\\nomic.ai\\\\GPT4All\\\\gpt4all-falcon-q4_0.gguf\"\n",
    "\n",
    "# HF_KEY = os.environ[\"HF_KEY\"]\n",
    "# os.environ['HUGGINGFACEHUB_API_TOKEN'] = HF_KEY\n",
    "\n",
    "# Setup embedding model for Vector DB\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "emb_kw_args = {\"device\":\"cuda\"}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL, model_kwargs=emb_kw_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper method that concats retrieved text for the prompt.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "## utility method to print text as part of the lang-chain chain\n",
    "def _print(text):\n",
    "    print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG and Non-RAG Langchain prompt template\n",
    "\n",
    "rag_template = \"\"\" \n",
    "            Answer only using the context.\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\n",
    "               \n",
    "            Answer: \"\"\"\n",
    "\n",
    "\n",
    "non_rag_template = \"\"\" \n",
    "            Answer the question: {question}\n",
    "               \n",
    "            Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the components of the chain\n",
    "\n",
    "# Prompt template for RAG request\n",
    "rag_prompt = PromptTemplate(template=rag_template, input_variables=['context','question'])\n",
    "\n",
    "# Prompt template for Non-RAG request\n",
    "non_rag_prompt = PromptTemplate(template=non_rag_template, input_variables=['question'])\n",
    "\n",
    "# Document pipeline\n",
    "\n",
    "# Load text from document source (directory of blog posts)\n",
    "dir_loader = DirectoryLoader(\"C:\\\\Users\\\\azaha\\\\py_code\\\\rag_test\\\\data\\\\posts\", glob=\"**/*.txt\", use_multithreading=True)\n",
    "blog_docs = dir_loader.load()\n",
    "\n",
    "# Initialise splitter and generate chunks (token chunking with 10% overlap)\n",
    "text_splitter = TokenTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=10)\n",
    "docs = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Initialise vector database (persisted Chroma) and associated retriever\n",
    "db = Chroma.from_documents(docs, embeddings, persist_directory=\"./data\")\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# Initialise GPT4ALL with ORCA model to run on local machine\n",
    "gpt4all = GPT4All(model=ORCA_MODEL_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Non-RAG chain\n",
    "non_rag_chain = LLMChain(\n",
    "    prompt=non_rag_prompt,\n",
    "    llm=gpt4all\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RAG chain - note the use of _print utility method which can be removed\n",
    "rag_chain = (\n",
    "    {\"context\":retriever | format_docs | _print, \"question\": RunnablePassthrough()} \n",
    "    | rag_prompt \n",
    "    | gpt4all\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main Run\n",
    "\n",
    "# Question database - feel free to modify\n",
    "question_db = [\n",
    "    \"What is InfluxDB?\",\n",
    "    \"What does 104+101 equal to?\",\n",
    "    \"What is the Tibco Action Processor?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box web-service) from which we need to 'extract' data, then we need to 'transform' it from source format to destination format (filtering, mapping etc.) and finally 'load' it into the destination (a file, a database, a black-box web-service).In many situations, using a commercial third-party data-load tool or a data-loading component integrated with the destination  (e.g. SQL*Loader)\n",
      "\n",
      "box web-service) from which we need to 'extract' data, then we need to 'transform' it from source format to destination format (filtering, mapping etc.) and finally 'load' it into the destination (a file, a database, a black-box web-service).In many situations, using a commercial third-party data-load tool or a data-loading component integrated with the destination  (e.g. SQL*Loader)\n",
      "\n",
      "box web-service) from which we need to 'extract' data, then we need to 'transform' it from source format to destination format (filtering, mapping etc.) and finally 'load' it into the destination (a file, a database, a black-box web-service).In many situations, using a commercial third-party data-load tool or a data-loading component integrated with the destination  (e.g. SQL*Loader)\n",
      "\n",
      "box web-service) from which we need to 'extract' data, then we need to 'transform' it from source format to destination format (filtering, mapping etc.) and finally 'load' it into the destination (a file, a database, a black-box web-service).In many situations, using a commercial third-party data-load tool or a data-loading component integrated with the destination  (e.g. SQL*Loader)\n",
      ")System.out.println(generateData(maxItemCount).parallel().filter(x -> x%2 == 0).noneMatch(x -> x%101 == 0)); // Result: true as there is no number between 1 and 100 that is divisible by 101}/*Generates whole numbers from 1 to limit parameter*/private static Stream<Integer> generateData(int limit) {return Stream.iterate(1, n->n+1).limit(limit\n",
      "\n",
      ")System.out.println(generateData(maxItemCount).parallel().filter(x -> x%2 == 0).noneMatch(x -> x%101 == 0)); // Result: true as there is no number between 1 and 100 that is divisible by 101}/*Generates whole numbers from 1 to limit parameter*/private static Stream<Integer> generateData(int limit) {return Stream.iterate(1, n->n+1).limit(limit\n",
      "\n",
      ")System.out.println(generateData(maxItemCount).parallel().filter(x -> x%2 == 0).noneMatch(x -> x%101 == 0)); // Result: true as there is no number between 1 and 100 that is divisible by 101}/*Generates whole numbers from 1 to limit parameter*/private static Stream<Integer> generateData(int limit) {return Stream.iterate(1, n->n+1).limit(limit\n",
      "\n",
      ")System.out.println(generateData(maxItemCount).parallel().filter(x -> x%2 == 0).noneMatch(x -> x%101 == 0)); // Result: true as there is no number between 1 and 100 that is divisible by 101}/*Generates whole numbers from 1 to limit parameter*/private static Stream<Integer> generateData(int limit) {return Stream.iterate(1, n->n+1).limit(limit\n",
      " of non-OS users for all processes. 0 is the Logical Machine ID, ALL signifies do it for all processes (otherwise add process name) and 0 is for the process instance.An easier way to check it is to install the Windows-based Workspace Browser (11.1.0) and point it to the new iProcess engine.NOTE: I have done a similar install on Windows 7 - 64bit edition. While TIBCO does not support iProcess Engine\n",
      "\n",
      " of non-OS users for all processes. 0 is the Logical Machine ID, ALL signifies do it for all processes (otherwise add process name) and 0 is for the process instance.An easier way to check it is to install the Windows-based Workspace Browser (11.1.0) and point it to the new iProcess engine.NOTE: I have done a similar install on Windows 7 - 64bit edition. While TIBCO does not support iProcess Engine\n",
      "\n",
      " of non-OS users for all processes. 0 is the Logical Machine ID, ALL signifies do it for all processes (otherwise add process name) and 0 is for the process instance.An easier way to check it is to install the Windows-based Workspace Browser (11.1.0) and point it to the new iProcess engine.NOTE: I have done a similar install on Windows 7 - 64bit edition. While TIBCO does not support iProcess Engine\n",
      "\n",
      " of non-OS users for all processes. 0 is the Logical Machine ID, ALL signifies do it for all processes (otherwise add process name) and 0 is for the process instance.An easier way to check it is to install the Windows-based Workspace Browser (11.1.0) and point it to the new iProcess engine.NOTE: I have done a similar install on Windows 7 - 64bit edition. While TIBCO does not support iProcess Engine\n",
      "RAG: \n",
      "            InfluxDB is a time series database that is optimized for collecting, storing, and querying large volumes of time-stamped data. It is designed to handle high-speed write and read operations and supports various data types such as integers, floating-point numbers, strings, and geospatial coordinates. InfluxDB also provides a flexible data model that allows for efficient storage and retrieval of data points over time. Non RAG: \n",
      "InfluxDB is a time-series database that is optimized for handling high volumes of data and performing real-time analytics. It is designed to manage and store time-stamped data points, which are often generated by sensors, applications, or other devices that track changes over time. InfluxDB supports SQL-like queries and offers various features such as continuous queries, retention policies, and replication capabilities.\n",
      "What is InfluxDB? --> 0.2096719300161407 tensor([[0.9780]], dtype=torch.float64)\n",
      "RAG: 205 Non RAG: 205\n",
      "What does 104+101 equal to? --> 0.0 tensor([[1.0000]], dtype=torch.float64)\n",
      "RAG: \n",
      "\n",
      "The Tibco Action Processor is an application that allows users to create and manage actions for processes in the iProcess engine. It enables non-OS users to perform tasks such as starting, stopping, or modifying processes without needing direct access to the Logical Machine ID. The Windows-based Workspace Browser (11.1.0) can be used to check and monitor the iProcess engine by pointing it to the Tibco Action Processor application. Non RAG: \n",
      "The TIBCO ActiveMatrix Service Bus (formerly known as TIBCO Rendezvous) is a messaging middleware product that enables reliable, secure and efficient communication between applications. It provides a publish-subscribe messaging model for asynchronous communication and supports various protocols such as HTTP, SMTP, FTP, and MQSeries. The Action Processor is an optional component of the TIBCO ActiveMatrix Service Bus that allows users to define custom actions or business logic to be executed when specific events occur on a message route. This can help automate complex processes, simplify application integration, and improve overall system performance.\n",
      "What is the Tibco Action Processor? --> 0.7654296990128492 tensor([[0.7071]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Comparing RAG/Non-RAG response\n",
    "rag_response = []\n",
    "rag_embed = []\n",
    "\n",
    "non_rag_response = []\n",
    "non_rag_embed = []\n",
    "\n",
    "for q in question_db:\n",
    "    rag_response.append(rag_chain.invoke(q))\n",
    "    rag_embed.append(np.array(embeddings.embed_query(rag_response[-1])))\n",
    "\n",
    "    non_rag_response.append(non_rag_chain.run(q))\n",
    "    non_rag_embed.append(np.array(embeddings.embed_query(non_rag_response[-1])))\n",
    "\n",
    "# Using sentence transformer to calculate embeddings and then using that to calculate similarity.\n",
    "for idx, r in enumerate(rag_embed):\n",
    "    nr = non_rag_embed[idx]\n",
    "    \n",
    "    print(\"RAG:\",rag_response[idx], \"Non RAG:\",non_rag_response[idx])\n",
    "    print(question_db[idx], \"-->\",np.linalg.norm(nr-r), dot_score(nr,r))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
